{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0ccb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eventid', 'iyear', 'imonth', 'iday', 'approxdate', 'extended', 'resolution', 'country', 'country_txt', 'region', 'region_txt', 'provstate', 'city', 'latitude', 'longitude', 'specificity', 'vicinity', 'location', 'summary', 'crit1', 'crit2', 'crit3', 'doubtterr', 'alternative', 'alternative_txt', 'multiple', 'success', 'suicide', 'attacktype1', 'attacktype1_txt', 'attacktype2', 'attacktype2_txt', 'attacktype3', 'attacktype3_txt', 'targtype1', 'targtype1_txt', 'targsubtype1', 'targsubtype1_txt', 'corp1', 'target1', 'natlty1', 'natlty1_txt', 'targtype2', 'targtype2_txt', 'targsubtype2', 'targsubtype2_txt', 'corp2', 'target2', 'natlty2', 'natlty2_txt', 'targtype3', 'targtype3_txt', 'targsubtype3', 'targsubtype3_txt', 'corp3', 'target3', 'natlty3', 'natlty3_txt', 'gname', 'gsubname', 'gname2', 'gsubname2', 'gname3', 'gsubname3', 'motive', 'guncertain1', 'guncertain2', 'guncertain3', 'individual', 'nperps', 'nperpcap', 'claimed', 'claimmode', 'claimmode_txt', 'claim2', 'claimmode2', 'claimmode2_txt', 'claim3', 'claimmode3', 'claimmode3_txt', 'compclaim', 'weaptype1', 'weaptype1_txt', 'weapsubtype1', 'weapsubtype1_txt', 'weaptype2', 'weaptype2_txt', 'weapsubtype2', 'weapsubtype2_txt', 'weaptype3', 'weaptype3_txt', 'weapsubtype3', 'weapsubtype3_txt', 'weaptype4', 'weaptype4_txt', 'weapsubtype4', 'weapsubtype4_txt', 'weapdetail', 'nkill', 'nkillus', 'nkillter', 'nwound', 'nwoundus', 'nwoundte', 'property', 'propextent', 'propextent_txt', 'propvalue', 'propcomment', 'ishostkid', 'nhostkid', 'nhostkidus', 'nhours', 'ndays', 'divert', 'kidhijcountry', 'ransom', 'ransomamt', 'ransomamtus', 'ransompaid', 'ransompaidus', 'ransomnote', 'hostkidoutcome', 'hostkidoutcome_txt', 'nreleased', 'addnotes', 'scite1', 'scite2', 'scite3', 'dbsource', 'INT_LOG', 'INT_IDEO', 'INT_MISC', 'INT_ANY', 'related']\n",
      "['197000000001', '1970', '7', '2', '0', '58', 'Dominican', 'Republic', '2', 'Central', 'America', 'Caribbean', 'Santo', 'Domingo', '18', '456792', '69', '951164', '1', '0', '1', '1', '1', '0', '0', '1', '0', '1', 'Assassination', '14', 'Private', 'Citizens', 'Property', '68', 'Named', 'Civilian', 'Julio', 'Guzman', '58', 'Dominican', 'Republic', 'MANO', 'D', '0', '0', '13', 'Unknown', '1', '0', '0', '0', '0', 'PGIS', '0', '0', '0', '0']\n",
      "['197000000002', '1970', '0', '0', '0', '130', 'Mexico', '1', 'North', 'America', 'Mexico', 'city', '19', '432608', '99', '133207', '1', '0', '1', '1', '1', '0', '0', '1', '0', '6', 'Hostage', 'Taking', 'Kidnapping', '7', 'Government', 'Diplomatic', '45', 'Diplomatic', 'Personnel', 'outside', 'of', 'embassy', 'consulate', 'Belgian', 'Ambassador', 'Daughter', 'Nadine', 'Chaval', 'daughter', '21', 'Belgium', '23rd', 'of', 'September', 'Communist', 'League', '0', '0', '7', '13', 'Unknown', '0', '0', '0', '1', '1', '0', 'Mexico', '1', '800000', 'PGIS', '0', '1', '1', '1']\n",
      "['197001000001', '1970', '1', '0', '0', '160', 'Philippines', '5', 'Southeast', 'Asia', 'Tarlac', 'Unknown', '15', '478598', '120', '599741', '4', '0', '1', '1', '1', '0', '0', '1', '0', '1', 'Assassination', '10', 'Journalists', 'Media', '54', 'Radio', 'Journalist', 'Staff', 'Facility', 'Voice', 'of', 'America', 'Employee', '217', 'United', 'States', 'Unknown', '0', '0', '13', 'Unknown', '1', '0', '0', '0', '0', 'PGIS', '9', '9', '1', '1']\n",
      "['197001000002', '1970', '1', '0', '0', '78', 'Greece', '8', 'Western', 'Europe', 'Attica', 'Athens', '37', '983773', '23', '728157', '1', '0', '1', '1', '1', '0', '0', '1', '0', '3', 'Bombing', 'Explosion', '7', 'Government', 'Diplomatic', '46', 'Embassy', 'Consulate', 'U', 'S', 'Embassy', '217', 'United', 'States', 'Unknown', '0', '0', '6', 'Explosives', 'Bombs', 'Dynamite', '16', 'Unknown', 'Explosive', 'Type', 'Explosive', '1', '0', '0', 'PGIS', '9', '9', '1', '1']\n",
      "['197001000003', '1970', '1', '0', '0', '101', 'Japan', '4', 'East', 'Asia', 'Fukouka', '33', '580412', '130', '396361', '1', '0', '1', '1', '1', '9', '0', '1', '0', '7', 'Facility', 'Infrastructure', 'Attack', '7', 'Government', 'Diplomatic', '46', 'Embassy', 'Consulate', 'U', 'S', 'Consulate', '217', 'United', 'States', 'Unknown', '0', '0', '8', 'Incendiary', 'Incendiary', '1', '0', '0', 'PGIS', '9', '9', '1', '1']\n",
      "['197001010002', '1970', '1', '1', '0', '217', 'United', 'States', '1', 'North', 'America', 'Illinois', 'Cairo', '37', '005105', '89', '176269', '1', '0', '1', '1', '1970', 'Unknown', 'African', 'American', 'assailants', 'fired', 'several', 'bullets', 'at', 'police', 'headquarters', 'in', 'Cairo', 'Illinois', 'United', 'States', 'There', 'were', 'no', 'casualties', 'however', 'one', 'bullet', 'narrowly', 'missed', 'several', 'police', 'officers', 'This', 'attack', 'took', 'place', 'during', 'heightened', 'racial', 'tensions', 'including', 'a', 'Black', 'boycott', 'of', 'White', 'owned', 'businesses', 'in', 'Cairo', 'Illinois', '1', '1', '1', '0', '0', '1', '0', '2', 'Armed', 'Assault', '3', 'Police', '22', 'Police', 'Building', 'headquarters', 'station', 'school', 'Cairo', 'Police', 'Department', 'Cairo', 'Police', 'Headquarters', '217', 'United', 'States', 'Black', 'Nationalists', 'To', 'protest', 'the', 'Cairo', 'Illinois', 'Police', 'Deparment', '0', '0', '99', '99', '0', '5', 'Firearms', '5', 'Unknown', 'Gun', 'Type', 'Several', 'gunshots', 'were', 'fired', '0', '0', '0', '0', '0', '0', '1', '3', 'Minor', 'likely', '1', 'million', '0', '0', 'The', 'Cairo', 'Chief', 'of', 'Police', 'William', 'Petersen', 'resigned', 'as', 'a', 'result', 'of', 'the', 'attack', 'Police', 'Chief', 'Quits', 'Washington', 'Post', 'January', '2', '1970', 'Cairo', 'Police', 'Chief', 'Quits', 'Decries', 'Local', 'Militants', 'Afro', 'American', 'January', '10', '1970', 'Christopher', 'Hewitt', 'Political', 'Violence', 'and', 'Terrorism', 'in', 'Modern', 'America', 'A', 'Chronology', 'Praeger', 'Security', 'International', '2005', 'Hewitt', 'Project', '9', '9', '0', '9']\n",
      "['197001020001', '1970', '1', '2', '0', '218', 'Uruguay', '3', 'South', 'America', 'Montevideo', 'Montevideo', '34', '891151', '56', '187214', '1', '0', '1', '1', '1', '0', '0', '0', '0', '1', 'Assassination', '3', 'Police', '25', 'Police', 'Security', 'Forces', 'Officers', 'Uruguayan', 'Police', 'Juan', 'Maria', 'de', 'Lucah', 'Chief', 'of', 'Directorate', 'of', 'info', 'and', 'intell', '218', 'Uruguay', 'Tupamaros', 'Uruguay', '0', '0', '3', '5', 'Firearms', '2', 'Automatic', 'Weapon', 'Automatic', 'firearm', '0', '0', '0', '0', '0', 'PGIS', '0', '0', '0', '0']\n",
      "['197001020002', '1970', '1', '2', '0', '217', 'United', 'States', '1', 'North', 'America', 'California', 'Oakland', '37', '805065', '122', '273024', '1', '0', 'Edes', 'Substation', '1', '2', '1970', 'Unknown', 'perpetrators', 'detonated', 'explosives', 'at', 'the', 'Pacific', 'Gas', 'Electric', 'Company', 'Edes', 'substation', 'in', 'Oakland', 'California', 'United', 'States', 'Three', 'transformers', 'were', 'damaged', 'costing', 'an', 'estimated', '20', '000', 'to', '25', '000', 'There', 'were', 'no', 'casualties', '1', '1', '1', '1', '2', 'Other', 'Crime', 'Type', '0', '1', '0', '3', 'Bombing', 'Explosion', '21', 'Utilities', '107', 'Electricity', 'Pacific', 'Gas', 'Electric', 'Company', 'Edes', 'Substation', '217', 'United', 'States', 'Unknown', '0', '0', '99', '99', '0', '6', 'Explosives', 'Bombs', 'Dynamite', '16', 'Unknown', 'Explosive', 'Type', '0', '0', '0', '0', '0', '0', '1', '3', 'Minor', 'likely', '1', 'million', '22500', 'Three', 'transformers', 'were', 'damaged', '0', '0', 'Damages', 'were', 'estimated', 'to', 'be', 'between', '20', '000', '25', '000', 'Committee', 'on', 'Government', 'Operations', 'United', 'States', 'Senate', 'Riots', 'Civil', 'and', 'Criminal', 'Disorders', 'U', 'S', 'Government', 'Printing', 'Office', 'August', '6', '1970', 'Christopher', 'Hewitt', 'Political', 'Violence', 'and', 'Terrorism', 'in', 'Modern', 'America', 'A', 'Chronology', 'Praeger', 'Security', 'International', '2005', 'Hewitt', 'Project', '9', '9', '0', '9']\n",
      "['197001020003', '1970', '1', '2', '0', '217', 'United', 'States', '1', 'North', 'America', 'Wisconsin', 'Madison', '43', '076592', '89', '412488', '1', '0', '1', '2', '1970', 'Karl', 'Armstrong', 'a', 'member', 'of', 'the', 'New', 'Years', 'Gang', 'threw', 'a', 'firebomb', 'at', 'R', 'O', 'T', 'C', 'offices', 'located', 'within', 'the', 'Old', 'Red', 'Gym', 'at', 'the', 'University', 'of', 'Wisconsin', 'in', 'Madison', 'Wisconsin', 'United', 'States', 'There', 'were', 'no', 'casualties', 'but', 'the', 'fire', 'caused', 'around', '60', '000', 'in', 'damages', 'to', 'the', 'building', '1', '1', '1', '0', '0', '1', '0', '7', 'Facility', 'Infrastructure', 'Attack', '4', 'Military', '28', 'Military', 'Recruiting', 'Station', 'Academy', 'R', 'O', 'T', 'C', 'R', 'O', 'T', 'C', 'offices', 'at', 'University', 'of', 'Wisconsin', 'Madison', '217', 'United', 'States', 'New', 'Year', 's', 'Gang', 'To', 'protest', 'the', 'War', 'in', 'Vietnam', 'and', 'the', 'draft', '0', '0', '1', '1', '1', '1', 'Letter', '8', 'Incendiary', '19', 'Molotov', 'Cocktail', 'Petrol', 'Bomb', 'Firebomb', 'consisting', 'of', 'gasoline', '0', '0', '0', '0', '0', '0', '1', '3', 'Minor', 'likely', '1', 'million', '60000', 'Basketball', 'courts', 'weight', 'room', 'swimming', 'pool', 'gymnastic', 'equipment', 'and', 'lockers', 'were', 'destroyed', '0', '0', 'The', 'New', 'Years', 'Gang', 'issue', 'a', 'communiqu√©', 'to', 'a', 'local', 'paper', 'claiming', 'that', 'they', 'perpetrated', 'this', 'attack', 'The', 'New', 'Years', 'Gang', 'previously', 'attempted', 'to', 'firebomb', 'the', 'R', 'O', 'T', 'C', 'building', 'a', 'week', 'earlier', 'As', 'a', 'result', 'of', 'the', 'attack', 'police', 'increased', 'their', 'presence', 'in', 'Madison', 'Karl', 'Armstrong', 's', 'girlfriend', 'Lynn', 'Schultz', 'drove', 'him', 'to', 'and', 'from', 'the', 'Old', 'Red', 'Gym', 'The', 'next', 'day', 'Armstrong', 'would', 'attempt', 'to', 'set', 'a', 'fire', 'in', 'the', 'Selective', 'Service', 'offices', 'in', 'Madison', '197001030001', 'Karl', 'Armstrong', 'would', 'be', 'captured', 'after', 'participating', 'in', 'the', 'deadly', 'bombing', 'of', 'Sterling', 'Hall', 'at', 'the', 'University', 'of', 'Wisconsin', 'on', 'August', '24', '1970', '197008240001', 'Tom', 'Bates', 'Rads', 'The', '1970', 'Bombing', 'of', 'the', 'Army', 'Math', 'Research', 'Center', 'at', 'the', 'University', 'of', 'Wisconsin', 'and', 'Its', 'Aftermath', 'HarperCollinsPublishing', '1992', 'David', 'Newman', 'Sandra', 'Sutherland', 'and', 'Jon', 'Stewart', 'The', 'Madison', 'Bomb', 'Story', 'The', 'Death', 'the', 'FBI', 'Saw', 'Heard', 'and', 'Won', 't', 'Talk', 'About', 'Mother', 'Jones', 'February', 'March', '1979', 'The', 'Wisconsin', 'Cartographers', 'Guild', 'Wisconsin', 's', 'Past', 'and', 'Present', 'A', 'Historical', 'Atlas', 'The', 'University', 'of', 'Wisconsin', 'Press', '2002', 'Hewitt', 'Project', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "# Load the CSV data\n",
    "with open('globalterrorismdb_0617dist.csv', encoding='ISO-8859-1') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = list(reader)\n",
    "\n",
    "# Tokenize the first 10 rows of data\n",
    "for row in data[:10]:\n",
    "    text = ' '.join(row)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60c563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\jerry\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jerry\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jerry\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\jerry\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\jerry\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jerry\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dfb8457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Define the URL to scrape\n",
    "url = 'https://www.olympic.org/athletes'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object from the response content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the athlete bio links\n",
    "athlete_links = soup.find_all('a', class_='link--nocolor athlete-list__link')\n",
    "\n",
    "# Loop through each athlete link and extract the biodata\n",
    "for link in athlete_links:\n",
    "    bio_url = 'https://www.olympic.org' + link['href']\n",
    "    bio_response = requests.get(bio_url)\n",
    "    bio_soup = BeautifulSoup(bio_response.content, 'html.parser')\n",
    "    name = bio_soup.find('h1', class_='page-header__title').text\n",
    "    nationality = bio_soup.find('div', class_='athlete-details__nationality').text\n",
    "    print(name, nationality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "766caa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mwordnet\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('wordnet')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\Jerry/nltk_data'\n",
      "    - 'C:\\\\Users\\\\Jerry\\\\anaconda3\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\Jerry\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\Jerry\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\Jerry\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jerry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jerry/nltk_data'\n    - 'C:\\\\Users\\\\Jerry\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jerry\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jerry\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jerry\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\decorators.py:35\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\blob.py:161\u001b[0m, in \u001b[0;36mWord.lemmatize\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     tag \u001b[38;5;241m=\u001b[39m \u001b[43m_wordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNOUN\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m _wordnet\u001b[38;5;241m.\u001b[39m_FILEMAP\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jerry/nltk_data'\n    - 'C:\\\\Users\\\\Jerry\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jerry\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jerry\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jerry\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Lemmatize the text\u001b[39;00m\n\u001b[0;32m     21\u001b[0m blob \u001b[38;5;241m=\u001b[39m TextBlob(standardized_text)\n\u001b[1;32m---> 22\u001b[0m lemmatized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word\u001b[38;5;241m.\u001b[39mlemmatize() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mwords])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_text)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Lemmatize the text\u001b[39;00m\n\u001b[0;32m     21\u001b[0m blob \u001b[38;5;241m=\u001b[39m TextBlob(standardized_text)\n\u001b[1;32m---> 22\u001b[0m lemmatized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mwords])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatized_text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\textblob\\decorators.py:38\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(err)\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingCorpusError()\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the text to be normalized, cleaned and lemmatized\n",
    "text = 'The quick brown Fox JUMPS over the laZy dog'\n",
    "\n",
    "# Convert text to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = text.split()\n",
    "cleaned_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Standardize by string replacement\n",
    "standardized_text = ' '.join(cleaned_words).replace('jumps', 'jump')\n",
    "\n",
    "# Lemmatize the text\n",
    "blob = TextBlob(standardized_text)\n",
    "lemmatized_text = ' '.join([word.lemmatize() for word in blob.words])\n",
    "\n",
    "print(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809efa12",
   "metadata": {},
   "outputs": [
    {
     "ename": "DeprecationError",
     "evalue": "PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDeprecationError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Tokenization using regular expressions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Open the PDF file\u001b[39;00m\n\u001b[0;32m     12\u001b[0m pdf_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe-great-gatsby.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m pdf_reader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDF2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPdfFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get the first page of the PDF\u001b[39;00m\n\u001b[0;32m     16\u001b[0m page \u001b[38;5;241m=\u001b[39m pdf_reader\u001b[38;5;241m.\u001b[39mgetPage(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PyPDF2\\_reader.py:1974\u001b[0m, in \u001b[0;36mPdfFileReader.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1974\u001b[0m     \u001b[43mdeprecation_with_replacement\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPdfFileReader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPdfReader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1976\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# maintain the default\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PyPDF2\\_utils.py:369\u001b[0m, in \u001b[0;36mdeprecation_with_replacement\u001b[1;34m(old_name, new_name, removed_in)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeprecation_with_replacement\u001b[39m(\n\u001b[0;32m    364\u001b[0m     old_name: \u001b[38;5;28mstr\u001b[39m, new_name: \u001b[38;5;28mstr\u001b[39m, removed_in: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    Raise an exception that a feature was already removed, but has a replacement.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     \u001b[43mdeprecation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEPR_MSG_HAPPENED\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremoved_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PyPDF2\\_utils.py:351\u001b[0m, in \u001b[0;36mdeprecation\u001b[1;34m(msg)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeprecation\u001b[39m(msg: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DeprecationError(msg)\n",
      "\u001b[1;31mDeprecationError\u001b[0m: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead."
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import PyPDF2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "\n",
    "# Tokenization using regular expressions\n",
    "# Open the PDF file\n",
    "pdf_file = open('the-great-gatsby.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "\n",
    "# Get the first page of the PDF\n",
    "page = pdf_reader.getPage(0)\n",
    "\n",
    "# Extract text from the page\n",
    "text = page.extractText()\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "print(\"Tokens from PDF file: \", tokens[:20])\n",
    "\n",
    "# Web scraping using bs4\n",
    "url = 'https://www.espn.com/olympics/summer/2021/athletes'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extracting biodata of top athletes\n",
    "biodata = []\n",
    "for athlete in soup.select('a[href*=\"/olympics/athletes/\"]'):\n",
    "    athlete_data = athlete.get_text(strip=True)\n",
    "    biodata.append(athlete_data)\n",
    "\n",
    "print(\"Biodata of top athletes:\\n\", biodata[:10])\n",
    "\n",
    "# Normalization and case conversion\n",
    "normalized_tokens = [token.lower() for token in tokens]\n",
    "print(\"Normalized tokens from PDF file: \", normalized_tokens[:20])\n",
    "\n",
    "# Stop words removal and standardization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "standardized_tokens = [Word(token).spellcheck()[0][0] for token in normalized_tokens if token not in stop_words]\n",
    "print(\"Standardized tokens from PDF file: \", standardized_tokens[:20])\n",
    "\n",
    "# Lemmatization using textblob\n",
    "lemmatized_tokens = [Word(token).lemmatize() for token in standardized_tokens]\n",
    "print(\"Lemmatized tokens from PDF file: \", lemmatized_tokens[:20])\n",
    "\n",
    "# Word frequency computation\n",
    "word_freq = {}\n",
    "for token in lemmatized_tokens:\n",
    "    if token in word_freq:\n",
    "        word_freq[token] += 1\n",
    "    else:\n",
    "        word_freq[token] = 1\n",
    "\n",
    "print(\"Word frequency: \", word_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11cba4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\jerry\\anaconda3\\lib\\site-packages (from PyPDF2) (4.1.1)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b25073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
